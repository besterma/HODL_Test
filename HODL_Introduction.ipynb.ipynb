{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "h5QiVwQxKJ_R",
        "uaFKUKGsGnmC",
        "9CoaVWTtHjN1",
        "S4gmO1GKilew",
        "33PSV9YLnxy4",
        "CX5xxzB1vXwi",
        "qLC-6JYK0Oji",
        "uri42H-24L80",
        "DtlSN3BgDObv",
        "LPpFsu8F59xI"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "*italicized text*# Hand-On Deep Learing - Introduction Session\n",
        "\n",
        "This session will introduce you to the basic concepts of differentiable programming and traning neural networks from scratch. We will be using the popular deep learning framework PyTorch.\n"
      ],
      "metadata": {
        "id": "FPXC6lGgqhJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "id": "2Ddy34LLCyOz",
        "outputId": "970c11d5-3d87-4689-9d39-2d485edbe39f"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-56-d5df0069828e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    121\u001b[0m       'TBE_EPHEM_CREDS_ADDR'] if ephemeral else _os.environ['TBE_CREDS_ADDR']\n\u001b[1;32m    122\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    124\u001b[0m         'request_auth', request={'authType': 'dfs_ephemeral'}, timeout_sec=None)\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    169\u001b[0m   request_id = send_request(\n\u001b[1;32m    170\u001b[0m       request_type, request, parent=parent, expect_reply=True)\n\u001b[0;32m--> 171\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    100\u001b[0m         reply.get('colab_msg_id') == message_id):\n\u001b[1;32m    101\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E2y1UqizqeUr"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will first talk about the bread-and-butter data type of deep learning - tensors. Once done, we will do a quick introduction to differentiable programs. On a simple example, we will show how to compute gradients in pytorch, and then lead you towards an implementation of a naive gradient descent training loop.\n",
        "\n",
        "Having got a better feel of how one trains the parameters of a differentiable program, we will introduce neural networks. We will provide you with a full implementation of a training loop, and help you train an approximation of a Boolean function.\n",
        "\n",
        "We will then steer away from illustrative examples and get started with practical, application-oriented deep learning. We will train both shallow and deep neural networks for image classification, tune the parameters of training and the sizes of architectures, and observe how the individual properties of our training setup influence the quality of the learning outcomes.\n",
        "\n",
        "We will conclude this session with the introduction of convolutional layers. The challenge of the day will be to use convolutional neural networks to correctly classify greyscale images of fashion articles. "
      ],
      "metadata": {
        "id": "TM0WBd5RpGGa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prelude: Tensors"
      ],
      "metadata": {
        "id": "kWg7Abwlot8z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For training of deep models, `torch` uses a special data type: `torch.tensor`. `torch.tensor` is an encapsulation of uniform nested lists that allows for some of these lists to be *trainable*, meaning that their values can be figured out in training on data.\n",
        "\n",
        "Before we can move on to do anything more exciting, one has to know a bit about tensors. All you need to know is that...."
      ],
      "metadata": {
        "id": "SsArlqzXFU1X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Tensors are an enhanced, uniform variant of multi-dimensional lists that torch operations can eat."
      ],
      "metadata": {
        "id": "ggXYIWqrF5gi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A = [[ 0.0, 1.0], [ 1.0 , 0.0]]\n",
        "\n",
        "try:\n",
        "  torch.matmul(A, A) # throws a TypeError\n",
        "except TypeError as error:\n",
        "  print(f\"An error occured in {error}\")"
      ],
      "metadata": {
        "id": "gI8gFtZwGOHy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "A_tensor = torch.tensor(A)\n",
        "\n",
        "torch.matmul(A_tensor, A_tensor)"
      ],
      "metadata": {
        "id": "qkXDlE2WGfCY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that nested lists that are candidates for tensors must be uniform in every dimension"
      ],
      "metadata": {
        "id": "o9veRCZCKlqp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "B = [\n",
        "    [[1, 2, 3], [4, 5, 6]],\n",
        "    [[0, 0], [1, 1]]\n",
        "]\n",
        "\n",
        "\n",
        "try:\n",
        "  B_tensor = torch.tensor(B) # throws a ValueError\n",
        "except ValueError as error:\n",
        "  print(f\"Could not form a tensor: {error}\")"
      ],
      "metadata": {
        "id": "pLXm_LvnLdGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise.** Create a tensor `I_tensor`, which is a 3x3 identity matrix."
      ],
      "metadata": {
        "id": "sLtbSji2NXIB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code here\n",
        "\n",
        "I = None \n",
        "\n",
        "I_tensor = None"
      ],
      "metadata": {
        "id": "M4a53MyvNnOZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Tensors have a multi-dimensional `size`, also known as `shape`\n",
        "The shape of a tensor describes the sizes of its individual tensor dimensions (also known as *axes*)."
      ],
      "metadata": {
        "id": "8N_bgQbmHt5x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A_tensor.shape"
      ],
      "metadata": {
        "id": "xp9ZCybhH9Fl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "A_tensor.size()"
      ],
      "metadata": {
        "id": "xNgPKohDIBZm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that `A` consists of two lists containing two elements each.\n",
        "Correspondingly, `A_tensor` has size of `[2, 2]`, meaning that `A_tensor` consists of to sub-tensors, namely `A_tensor[0]` and `A_tensor[1]`, containing two elements each."
      ],
      "metadata": {
        "id": "x2rzIVf0IRDl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A_tensor[0]"
      ],
      "metadata": {
        "id": "c9UQIDJMI7IZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we take `B` such that `B` contains two lists of lists, such as in"
      ],
      "metadata": {
        "id": "HqgN1NydJJfC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "B = [\n",
        "    [[1, 2, 3], [4, 5, 6]],\n",
        "    [[0, 0, 0], [1, 1, 1]]\n",
        "]"
      ],
      "metadata": {
        "id": "ZAJfXvqlIZra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we can turn it into a `B_tensor` of size `[2,2,3]`,"
      ],
      "metadata": {
        "id": "Hd4A95w0JSHw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "B_tensor = torch.tensor(B)\n",
        "B_tensor.shape"
      ],
      "metadata": {
        "id": "X6KrSZ3aJkzt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "... meaning that `B_tensor` consits of two two-dimensional sub-tensors, `B_tensor[0]`, and `B_tensor[1]`."
      ],
      "metadata": {
        "id": "6drfln9LJrRW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise.** What should be the shape of `I_tensor`?"
      ],
      "metadata": {
        "id": "H1IUoaVnNwL1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "I_tensor_shape_intended = torch.Size( [  ] ) # modify this line with your guess"
      ],
      "metadata": {
        "id": "jBaWraIFN0xF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise.** Retrieve the shape of `I_tensor`."
      ],
      "metadata": {
        "id": "7-KzK5SbOMG2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "I_tensor_shape = None # modify this line with the correct code"
      ],
      "metadata": {
        "id": "E9khgv9NORPi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise.** Are they the same?"
      ],
      "metadata": {
        "id": "PujSmcTuOXVE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this code block.\n",
        "\n",
        "if I_tensor_shape_intended == I_tensor_shape:\n",
        "  print(\"I_tensor has shape as intended.\")\n",
        "else:\n",
        "  print(\"I_tensor does not have shape as intended.\")"
      ],
      "metadata": {
        "id": "kNlLXAAQOgj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### You can access arbitrary sub-tensors of every tensor\n",
        "\n",
        "For this, you can use the python's usual slicing notation. For example, to get the second element of each of the deepest lists of `B` in the corresponding tensor, one can simply write"
      ],
      "metadata": {
        "id": "h5QiVwQxKJ_R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "B_tensor[:,:,1]"
      ],
      "metadata": {
        "id": "c1PnNsaaL_Ij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Tensors can be either trainable or non-trainable\n",
        "\n",
        "The trainable tensors are the ones that have `require_gradient` set to `True`."
      ],
      "metadata": {
        "id": "uaFKUKGsGnmC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A_trainable = torch.tensor(A, requires_grad=True)\n",
        "\n",
        "print(f\"A_tensor is trainable: {A_tensor.requires_grad}\")\n",
        "print(f\"A_trainable is trainable: {A_trainable.requires_grad}\")"
      ],
      "metadata": {
        "id": "UZrpvNetHHks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Tensors can be used in computations element-wise, as long as the dimensions match"
      ],
      "metadata": {
        "id": "9CoaVWTtHjN1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "B_tensor + B_tensor ** 2 - 0.3 * B_tensor"
      ],
      "metadata": {
        "id": "vdFQUCmzHijV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise.** With the help of PyTorch documentation online, find the square root of $B^3$."
      ],
      "metadata": {
        "id": "Z5Ndj7QtUitM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your solution\n"
      ],
      "metadata": {
        "id": "WmGpu-sjZrNQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Programs"
      ],
      "metadata": {
        "id": "0lKIe3UBGO_U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You are certainly familiar with the notion of a classical computer *program*. For our purposes, a program $f$ is an information processing device that takes some inputs $x$ and produces outputs $f(x)$.\n",
        "\n",
        "Programs can be *pure*, meaning they have no side effects"
      ],
      "metadata": {
        "id": "rwUqmRDnwStr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def is_large(x):\n",
        "  threshold = 10\n",
        "  if x > threshold:\n",
        "    return True\n",
        "  else:\n",
        "    return False"
      ],
      "metadata": {
        "id": "W0SHjFoVxZmr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "... or \"impure\", meaning that executing them alters some fixed memory state in the computer."
      ],
      "metadata": {
        "id": "UMJHOcUyxnn1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "large_number_count = 0\n",
        "\n",
        "def impure_is_large(x):\n",
        "  threshold = 10\n",
        "  if x > threshold:\n",
        "    large_number_count += 1\n",
        "    return True\n",
        "  else:\n",
        "    return False"
      ],
      "metadata": {
        "id": "8yH6g2acx11M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that the variable `threshold` in both `is_large` and `impure_is_large` does not really encode a state of the program, but is a parameter determining which numbers will and which numbers won't be considered \"large\".\n",
        "\n",
        "Throughout this session, we will only be dealing with pure programs."
      ],
      "metadata": {
        "id": "XRzZXpCFxvsK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Differentiable Programs and Why They Are So Special\n",
        "\n",
        "The entire world is now interested in a particular sub-class of programs, called *differentiable programs*."
      ],
      "metadata": {
        "id": "U6JZJgF0yR5o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A differentiable program is a program $f$ such that $f$ is differentiable with respect to its parameters. Here is an example of a differentiable program $f$ taking $x$ as input and multiplying it by a parameter $p$."
      ],
      "metadata": {
        "id": "11getKqlvbJE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "p = torch.tensor([ 1.0 ], requires_grad=True)\n",
        "\n",
        "def f(x):\n",
        "  global p\n",
        "  return p * x"
      ],
      "metadata": {
        "id": "l0fCXWtzv6et"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apart from forcing software engineers to dust off their high-school calculus knowledge, what are these differentiable programs actually good for? Why has the entire software engineering and data science world gone crazy over them?"
      ],
      "metadata": {
        "id": "AEV5JmM5v8IG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We won't keep you in suspense, here's the \"secret\":\n",
        "\n",
        "> Given input-output data, differentiable programs can be taught, through trial and error, to use the right parameters.\n",
        "\n",
        "So, in the example of `f` above, we could train the program to learn the \"true\" value of `p`."
      ],
      "metadata": {
        "id": "MWwo3vb00uaO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The method enabling this is the one of gradient descent training. This has been covered well in many lectures and online resources. If you need a quick refresher, have a look through at the corresponding videos from the Computational Thinking course, available [here](https://)."
      ],
      "metadata": {
        "id": "mfHkDnBA1km6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gradient Computation in PyTorch\n",
        "At the helm of gradient-descent training in PyTorch is the `autograd` module. `torch.autograd` is PyTorch's automatic differentiation engine that powers gradient-descent training.\n",
        "\n",
        "Suppose you take some trainable tensor $x$ and pass it through $f$."
      ],
      "metadata": {
        "id": "S4gmO1GKilew"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor([ 5.2 ], requires_grad=True)\n",
        "output = f(x)\n",
        "output"
      ],
      "metadata": {
        "id": "ovOmObj-aCnr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that `output` now has an additional field, `grad_fn`, that was set by the `autograd` system to keep track of what operations have been performed on `x` to arrive at output."
      ],
      "metadata": {
        "id": "QcDXcjV3aTK5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, given some expected output for $f(x)$, say $1$, `autograd` allows you to compute an indication of how `p` needs to be changed in order for $f(x)$ to eventually yield the correct outputs. This is done in a process called *backward pass*."
      ],
      "metadata": {
        "id": "DKlQaJ9Ialbh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "expected_output = torch.tensor([1.0])\n",
        "loss = (output - expected_output) ** 2\n",
        "loss.backward(retain_graph=True)"
      ],
      "metadata": {
        "id": "V6nNrPt4bLbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This indication can then by inspected by asking `p` what its gradient is by reading `p.grad`."
      ],
      "metadata": {
        "id": "M6I-wj_QbV83"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "p.grad"
      ],
      "metadata": {
        "id": "_YanreQybVVa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This indication can be interpreted as\n",
        "\n",
        "> Decreasing `p` by some small $\\epsilon$ will decrease the loss by $87.36\\epsilon$."
      ],
      "metadata": {
        "id": "2vNxdfQgcNcq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "And, as you already know, leveraging the negation of gradient as the indication of the direction in which one should modify the parameters in order to descent towards lower values of the loss, gradient descent training is simply the routine under which one iteratively computes and then applies the gradient of the loss function with respect to parameters of the computation to minimise the loss."
      ],
      "metadata": {
        "id": "xVLVUzhpjQOS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise.** Fill in the code below to compute the gradients for `p` equal to `0.75`, `0.5`, `0.25`, and `0.20`. What do you observe?"
      ],
      "metadata": {
        "id": "s93DpbnznITx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# case p = 0.75\n",
        "p = torch.tensor([ 0.75 ], requires_grad=True)\n",
        "\n",
        "#  - you want something gradienty here :)\n",
        "\n",
        "print(f\"For p = 0.75, the gradient is {p.grad}\")\n",
        "\n",
        "# case p = 0.50\n",
        "p = torch.tensor([ 0.50 ], requires_grad=True)\n",
        "\n",
        "#  - also here\n",
        "\n",
        "print(f\"For p = 0.50, the gradient is {p.grad}\")\n",
        "\n",
        "# case p = 0.25\n",
        "p = torch.tensor([ 0.25 ], requires_grad=True)\n",
        "\n",
        "#  - ...\n",
        "\n",
        "print(f\"For p = 0.25, the gradient is {p.grad}\")\n",
        "\n",
        "# case p = 0.20\n",
        "p = torch.tensor([ 0.20 ], requires_grad=True)\n",
        "\n",
        "print(f\"For p = 0.20, the gradient is {p.grad}\")"
      ],
      "metadata": {
        "id": "ZoWVAgtCnlT6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A Basic Training Loop\n",
        "As hinted on by the above example, modifying the parameters of a differentiable program in the direction opposite to its gradient (i.e. in the direction in which the loss decreases most rapidly) generally guides the differentiable programs towards a minimum in the loss.\n",
        "\n",
        "This process can be repeated iteratively, to form what is called a *training loop*. A typical training procedure of a differentiable program looks as follows:\n",
        "\n",
        "\n",
        "1. Initialise the parameters of the differentiable program according to an appropriate scheme.\n",
        "2. Take the inputs provided and perform a *forward pass* -- apply the program to the inputs.\n",
        "3. Compute the loss between the expected outputs and the actual outputs of the program.\n",
        "4. Compute the gradient of the loss with respect to the program's parameters.\n",
        "5. Scale the gradients by the desired pace of descent -- *the learning rate** -- and update the parameters accordingly.\n",
        "6. If not done yet, go back to 2..\n",
        "\n",
        "\n",
        "You already possess all the basic ingredients necessary to implement such a training procedure yourself. Let's do that."
      ],
      "metadata": {
        "id": "33PSV9YLnxy4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise.** Fill in the code below to arrive at a working implementation of a gradient descent training loop for $f$.\n"
      ],
      "metadata": {
        "id": "_nYngzhdoCRt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_f(x, y, learning_rate: float = 0.01, number_of_iterations: int = 50):\n",
        "  global p\n",
        "  # TODO: initialise p to a random tensor between 0 and 1 (hint: use torch.rand)\n",
        "  # remember that you need p to be trainable!\n",
        "\n",
        "  for iteration in range(1, number_of_iterations+1):\n",
        "    # TODO: perform a \"forward pass\" (apply f to x)\n",
        "    output = None\n",
        "    \n",
        "    # compute the loss\n",
        "    loss = torch.sum((output - y) ** 2) / output.size(0)\n",
        "\n",
        "    # TODO: compute the gradient of `loss` given p\n",
        "\n",
        "    # TODO: subtract learning_rate*(gradient of p) from p ...\n",
        "    with torch.no_grad():\n",
        "      # ... here\n",
        "\n",
        "    # finally, erase the gradients for the next iteration\n",
        "    p.grad.data.zero_()"
      ],
      "metadata": {
        "id": "dn1YONH9lr5r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise.** Choose a value of `p_true` -- the parameter value for $f$ to be learned. Then, run the code below to check the correctness of your training loop from above. If you struggle to get the right answer, consider decreasing the learning rate and increasing the number of iterations."
      ],
      "metadata": {
        "id": "4mmw4UKFl5Kh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "p_true = None # the parameter value of p to be learned\n",
        "datapoint_count = None # the number of datapoints to use for training in every iteration of `train_f`\n",
        "\n",
        "x = torch.rand((datapoint_count,))\n",
        "y = x * p_true\n",
        "\n",
        "train_f(x, y)\n",
        "print(f\"The true value is {p_true.item()}, the value learned by gradient descent is {p.item()}\")"
      ],
      "metadata": {
        "id": "9xA81E0Ll1N5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introducing Neural Networks\n",
        "Neural networks are a particular class of differentiable programs, for which it has been theoretically proven that they can learn to approximate an arbitrary integrable function arbitrarily well, as long as they are given enough *representational power*.\n",
        "\n",
        "*Here is where the deep learning black magic begins.* \n",
        "\n",
        "Classical programs consist of a sequence of specific operations such as addition or conditional value assignment. Neural networks are differentiable programs that consist of a sequence of amenable elementary building blocks, traditionally referred to as *layers*, that can ultimately perform a wide variety of operations. The \"bigger\" these layers are, the more complex behaviour they can learn to exhibit.\n",
        "\n",
        "There exists several popular types of neural network building blocks, including the trainable *linear layer*, or the non-trainable *activation*, *softmax*, and *dropout layers*, to name but a few. The combination of a linear layer and an activation layer is sometimes referred to as *dense layer* and is the basic building block of a *deep neural network*.\n",
        "\n",
        "The amount of *representational power* network has is determined by the sizes of its trainable layers. Linear layers have a \"width\" (the number of constituent neurons). The wider the layer, the more fine-grained operation it is capable of representing. Whether it can learn to represent this operation is, however, an entirely different question."
      ],
      "metadata": {
        "id": "3PXg-iPLsNLV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Constructing Neural Networks\n",
        "Without further ado, let use these building blocks to form neural networks.\n",
        "\n",
        "Knowing the format of the operation of individual layers, you could go ahead and implement them directly. To avoid uncanny detail, we will instead use the ready-made implementations of these layers from the `torch.nn` module.\n",
        "\n",
        "In general, the [documentation](https://pytorch.org/docs/stable/nn.html) of the `torch.nn` module is what you want to turn to to understand a new layer type.\n",
        "\n",
        "We walk you through creating instances of various layer types in the code below. We directly use the instances to operate input data."
      ],
      "metadata": {
        "id": "oSgEwtUzQSEq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Linear Layers\n",
        "\n",
        "Let us begin with the most basic layer in deep learning, the Linear layer."
      ],
      "metadata": {
        "id": "CX5xxzB1vXwi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# construct a linear layer that takes a tensor of size (3,) and produces a \n",
        "#  tensor of size (5,)\n",
        "linear_layer = torch.nn.Linear(3, 5)\n",
        "\n",
        "# pass [1, 2, 3] through the layer\n",
        "example_input = torch.tensor([ 1, 2, 3 ], dtype=torch.float)\n",
        "linear_layer(example_input)"
      ],
      "metadata": {
        "id": "9dUDE6jjQRQW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that as promised in the call to `nn.Linear(3, 5)`, the output tensor has 5 entries. Its output values are the result of an internal state (the layer *weights*) that has been initialised at random."
      ],
      "metadata": {
        "id": "sqrHdPdHvehw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Activation Layers\n",
        "\n",
        "Several types of activation layers exist, most notably the logistic sigmoid, rectified linear unit (ReLU), and the hyperbolic tangent. Each of these has a layer in `torch.nn`."
      ],
      "metadata": {
        "id": "3SQI0UMFwxj2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# construct a ReLU layer\n",
        "relu_layer = torch.nn.ReLU()\n",
        "\n",
        "# pass [ -3, -2, -1, 0, +1, +2, +3 ] through the ReLU layer\n",
        "example_input = torch.tensor([ -3, -2, -1, 0, +1, +2, +3 ], dtype=torch.float)\n",
        "relu_layer(example_input)"
      ],
      "metadata": {
        "id": "J7tTJxVGu1wS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# construct a sigmoid layer\n",
        "sigmoid_layer = torch.nn.Sigmoid()\n",
        "\n",
        "# pass [ -3, -2, -1, 0, +1, +2, +3 ] through the sigmoid layer\n",
        "sigmoid_layer(example_input)"
      ],
      "metadata": {
        "id": "st3FTSinzry1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# construct a tanh layer\n",
        "tanh_layer = torch.nn.Tanh()\n",
        "\n",
        "# pass [ -3, -2, -1, 0, +1, +2, +3 ] through the tanh layer\n",
        "tanh_layer(example_input)"
      ],
      "metadata": {
        "id": "VHPmBuUdz4xL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, going from minus infinity towards infinity around 0, the ReLU transits from constant 0 to linear behaviour at 0, the logistic sigmoid proceeds to climb from 0 towards 1, and tanh climbs from -1 towards +1."
      ],
      "metadata": {
        "id": "tvrPbIhrz7J1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Dropout Layers\n",
        "\n",
        "It is sometimes to the advantage of model training to \"drop out\" some of the incoming values at random. To this end, `torch.nn` provides the `Dropout` layer, which can be parametrised at construction with the probability of an input value being dropped out."
      ],
      "metadata": {
        "id": "qLC-6JYK0Oji"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# construct a dropout layer with probability 0.0\n",
        "dropout_layer = torch.nn.Dropout(p=0.5)\n",
        "\n",
        "# pass [ -3, -2, -1, 0, +1, +2, +3 ] through the dropout layer\n",
        "dropout_layer(example_input)\n",
        "\n",
        "# with p=0.5, roughly half of the inputs should be dropped out on average, \n",
        "#  and the remaining outputs are scaled up by 1/(1-p) == 2 \n",
        "# run this snippet multiple times to observe the effects of random dropout"
      ],
      "metadata": {
        "id": "8PLHSgNbz54s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Softmax\n",
        "Sometimes we wish to interpret an $n$-dimensional vector of real values as scores in favour of a single one of $n$ discrete elements possessing a certain property. To this end, we often use the \"softmax\" layer.\n",
        "\n",
        "The softmax layer takes the $n$-dimensional vector of real values and produces an $n$-dimensional vector of values between $0$ and $1$, whose individual entries sum up to $1$.\n",
        "\n",
        "The bigger an entry of the input vector is relative to other entries, the closer its corresponding value in the output vector is to $1$."
      ],
      "metadata": {
        "id": "uri42H-24L80"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# construct a softmax layer\n",
        "softmax_layer = torch.nn.Softmax(dim=0)\n",
        "\n",
        "# pass [ -3, -2, -1, 0, +1, +2, +3 ] through the softmax layer\n",
        "softmax_layer(example_input)"
      ],
      "metadata": {
        "id": "Lv4tVwYK3WZc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pass [ -1, 2, 5, 100 ] through the softmax layer\n",
        "softmax_layer(torch.tensor([ -1, 2, 5, 100 ], dtype=torch.float))"
      ],
      "metadata": {
        "id": "rvmT75wPoiYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Putting the Layers Together -- Implementing a DNN\n"
      ],
      "metadata": {
        "id": "3hF_z6JC4M_V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Neural networks are graphs of layers. In PyTorch, we generally tend to implement our neural networks as classes whose constructors construct the constituent parts of the network, and whose `forward` function passes the data through these parts.\n",
        "\n",
        "Below is an example implementation of a shallow neural network. This network is *shallow* as it contains only one hidden trainable layer (=layer that is not an input or output layer).\n"
      ],
      "metadata": {
        "id": "CW_XVRYLuLJG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ShallowNeuralNet(nn.Module):\n",
        "    def __init__(self, input_width: int, hidden_layer_width: int, output_width):\n",
        "        super().__init__()\n",
        "        self.hidden_layer = nn.Linear(input_width, hidden_layer_width)\n",
        "        self.hidden_relu = nn.ReLU()\n",
        "        self.output_layer = nn.Linear(hidden_layer_width, output_width)\n",
        "\n",
        "    def forward(self, input):\n",
        "        hidden_trainable_output = self.hidden_layer(input)\n",
        "        hidden_relu_output = self.hidden_relu(hidden_trainable_output)\n",
        "        output = self.output_layer(hidden_relu_output)\n",
        "        return output"
      ],
      "metadata": {
        "id": "8boLlg2HwJhg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once the network behaviour has been described in this fashion, we can create an instance of the entire network at once and use it to process data in exactly the same way as we would use layers."
      ],
      "metadata": {
        "id": "Le9mkI0Lwz0i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shallow_nn_instance = ShallowNeuralNet(5, 10, 2)\n",
        "\n",
        "example_input = torch.ones(5)\n",
        "shallow_nn_instance(example_input)"
      ],
      "metadata": {
        "id": "jjQx_ztbwzGG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise.** Fill in the snippet below to arrive at an implementation of a deep ReLU neural networks with layer profile given by a list of integers."
      ],
      "metadata": {
        "id": "AxG85zD10uCD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DeepNeuralNet(nn.Module):\n",
        "  def __init__(self, input_width, hidden_layer_profile, output_width, output_activation=None):\n",
        "    super().__init__()\n",
        "    self.layers = nn.ModuleList()\n",
        "\n",
        "    # create the first hidden layer\n",
        "    self.layers.append(nn.Linear(input_width, hidden_layer_profile[0]))\n",
        "    self.layers.append(nn.ReLU())\n",
        "\n",
        "    # create the internal hidden layers\n",
        "    for in_width, out_width in zip(hidden_layer_profile[0:-1], hidden_layer_profile[1:]):\n",
        "      self.layers.append(nn.Linear(in_width, out_width))\n",
        "      self.layers.append(nn.ReLU())\n",
        "\n",
        "    # create the output layer\n",
        "    self.output_layer = nn.Linear(hidden_layer_profile[-1], output_width)\n",
        "    self.output_activation = nn.Identity() if not output_activation else output_activation\n",
        "  \n",
        "  def forward(self, input):\n",
        "    x = input\n",
        "\n",
        "    # loop through the layers to produce the output of the hidden network\n",
        "    for layer in self.layers:\n",
        "      # TODO: pass the intermediate output of the previous layer through the current layer\n",
        "      x = None\n",
        "\n",
        "    # TODO: produce the output of the network from the intermediate output of the last hidden layer\n",
        "    output_before_activation = None\n",
        "\n",
        "    # TODO: engage the optional activation in self.output_activation on the output_before_activation\n",
        "    output = None\n",
        "\n",
        "    return output"
      ],
      "metadata": {
        "id": "GIP3GTxM0tZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise.** Test the class for generic deep neural networks below. Does everything work as expected?\n"
      ],
      "metadata": {
        "id": "5ctV1s9_2lw_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# try passing a tensor of random numbers through the network\n",
        "input1 = torch.rand((10,))\n",
        "deep_nn_instance = DeepNeuralNet(input_width=10, hidden_layer_profile=[10, 7, 5], output_width=1)\n",
        "\n",
        "deep_nn_instance(input1)"
      ],
      "metadata": {
        "id": "ApoMITVs2wLd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A typical neural network design pattern appearing in search and recommender systems is that of \"two towers\". Explained in brief, the network consists of two separate sub-networks, one for queries and one for results. In some special cases when the modalities of the queries and results are the same, the towers can be made to \"share weights\" (use the same architecture and parameters to process their respective inputs). In such case, one might talk of the \"twin tower\" architecture being used.\n",
        "\n",
        "**Exercise (Weight Sharing).** Using the class `DeepNeuralNet` you implemented above, fill in the code below to produce an implementation of the twin tower architecture."
      ],
      "metadata": {
        "id": "NOafensI3MNS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TwinDeepNeuralNet(nn.Module):\n",
        "  def __init__(self, input_width, hidden_layer_profile, output_width):\n",
        "    super().__init__()\n",
        "    \n",
        "    # TODO: use DeepNeuralNet to construct a network that can perform the function of a \"twin tower\" network\n",
        "\n",
        "  def forward(self, input):\n",
        "    # identify the query and the value as sub-tensors of the input tensor\n",
        "    input_query = input[0,:]\n",
        "    input_value = input[1,:]\n",
        "\n",
        "    # TODO: use the layer(s) or sub-network(s) initialised in the constructor to implement the functionality of a \"twin tower\" network\n",
        "    output_query = None\n",
        "    output_value = None\n",
        "\n",
        "\n",
        "    # form a single output tensor as a disjoint union of the query and value tensors\n",
        "    output_query = output_query.unsqueeze(0)\n",
        "    output_value = output_value.unsqueeze(0)\n",
        "    output = torch.cat([output_query, output_value], dim=0)\n",
        "\n",
        "    return output"
      ],
      "metadata": {
        "id": "49VQyDDT5I6m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On the simple example below, test whether your implementation works."
      ],
      "metadata": {
        "id": "zaiTPH-i9etC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "testing_input = torch.ones((2, 5))\n",
        "\n",
        "twin_nn_instance = TwinDeepNeuralNet(5, [10, 10], 1)\n",
        "testing_output = twin_nn_instance(testing_input)\n",
        "\n",
        "equality = torch.all(testing_output[0] == testing_output[1])\n",
        "\n",
        "print(f\"The outputs of the twins are {'equal' if equality else 'not equal'}\")"
      ],
      "metadata": {
        "id": "Yx19kOVi9hWG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A Training Loop for Neural Networks\n",
        "In a previous section concerning differentiable programs, we introduced the intuition for using the gradient information due to a choice of loss function to find optimal parameters of a differentiable program. \n",
        "\n",
        "This is exactly what we do for neural networks as well in order to train them to have the behaviour we desire of them.\n",
        "\n",
        "We give code for optimisation of a neural network `net` with particular loss function `loss` on dataset loaded by a `dataloader` below, and comment on it step by step."
      ],
      "metadata": {
        "id": "HqrY3Z77-TYq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def training_loop(dataloader, net, loss_fn, optimiser, verbosity=3):\n",
        "    size = len(dataloader.dataset)\n",
        "    last_print_point = 0\n",
        "    current = 0\n",
        "\n",
        "    acc_loss = 0\n",
        "    acc_count = 0\n",
        "\n",
        "    # for every slice (X, y) of the training dataset\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X = X.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        # perform a forward pass to compute the outputs of the net\n",
        "        pred = net(X)\n",
        "\n",
        "        # calculate the loss between the outputs of the net and the desired outputs\n",
        "        loss_val = loss_fn(pred, y)\n",
        "        acc_loss += loss_val.item()\n",
        "        acc_count += 1\n",
        "\n",
        "        # zero the gradients computed in the previous step \n",
        "        optimiser.zero_grad()\n",
        "\n",
        "        # calculate the gradients of the parameters of the net\n",
        "        loss_val.backward()\n",
        "\n",
        "        # use the gradients to update the weights of the network\n",
        "        optimiser.step()\n",
        "\n",
        "        # compute how many datapoints have already been used for training\n",
        "        current = batch * len(X)\n",
        "\n",
        "        # report on the training progress roughly every 10% of the progress\n",
        "        if verbosity >= 3 and (current - last_print_point) / size >= 0.1:\n",
        "            loss_val = loss_val.item()\n",
        "            last_print_point = current\n",
        "            print(f\"loss: {loss_val:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "    return acc_loss / acc_count"
      ],
      "metadata": {
        "id": "0GWPK-epiM3Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now possess all the tools necessary for constructing simple neural networks and for training them towards some particular behaviour by gradient descent loss minimisation. Let us put these tools to good use."
      ],
      "metadata": {
        "id": "o84sL9H2pNR2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Learning a Boolean Function\n",
        "\n"
      ],
      "metadata": {
        "id": "7ogK_KsxYZDN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us consider the problem of learning a random Boolean function $f: \\left\\{ 0,1 \\right\\}^n \\to \\left\\{ 0,1 \\right\\}^n$. It might sound a bit dry at first, but bear with us, it is a very natural and tractable example for the examination of the representational power of various types of neural networks."
      ],
      "metadata": {
        "id": "MAPqtEvW3Kzj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_binary_array(number: int, length: int) -> list:\n",
        "\treturn [ (number>>k)&1 for k in range(0, length) ]\n",
        "\n",
        "data_x_list = []\n",
        "\n",
        "for i in range(0, 256):\n",
        "  data_x_list.append(make_binary_array(i, 8))\n",
        "\n",
        "data_x = torch.tensor(data_x_list, dtype=torch.float)\n",
        "data_y = torch.randint(low=0, high=2, size=(256, 8)).type(torch.float)\n",
        "dataset = torch.utils.data.TensorDataset(data_x, data_y)"
      ],
      "metadata": {
        "id": "CVoL-jQFYnDi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above code generates a dataset of $(x, y)$ pairs where $x$ is any $n=8$-bit signal and $y = f(x)$, with $f$ chosen uniformly at random. "
      ],
      "metadata": {
        "id": "5FqdC15d3gkg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given these examples, can we learn a neural network that performs the function of $f$? Yes! Just run the code below"
      ],
      "metadata": {
        "id": "ZTVEVnlMBrCt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "net = DeepNeuralNet(input_width=8, hidden_layer_profile=[256], output_width=8, output_activation=nn.Sigmoid())"
      ],
      "metadata": {
        "id": "J-FDh-msFG7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def testing_loop(dataloader, net):\n",
        "  size = len(dataloader.dataset)\n",
        "  last_print_point = 0\n",
        "  current = 0\n",
        "\n",
        "  acc_correct = 0\n",
        "  acc_count = 0\n",
        "\n",
        "  # for every slice (X, y) of the training dataset\n",
        "  with torch.no_grad():\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X = X.to(device)\n",
        "        y = y.to(device)\n",
        "        \n",
        "        # perform a forward pass to compute the outputs of the net\n",
        "        pred = net(X)\n",
        "\n",
        "        # round the predictions (0 - 0.5 towards zero, >0.5 towards one)\n",
        "        pred_rounded = torch.round(pred)\n",
        "\n",
        "        # compute the number of correct entries\n",
        "        acc_correct += torch.count_nonzero(pred_rounded == y).item()\n",
        "        acc_count += y.numel()\n",
        "\n",
        "  return acc_correct / acc_count"
      ],
      "metadata": {
        "id": "JAUZSwWBLyd4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(dataloader, net, loss_fn, optimiser, epochs, verbosity=3):\n",
        "  least_loss = None\n",
        "\n",
        "  for t in range(epochs):\n",
        "    if verbosity >= 3:\n",
        "      print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    \n",
        "    mean_loss = training_loop(dataloader, net, loss_fn, optimiser, verbosity=verbosity)\n",
        "    accuracy = testing_loop(dataloader, net)\n",
        "    if not least_loss or mean_loss < least_loss:\n",
        "      least_loss = mean_loss\n",
        "    if verbosity >= 2:\n",
        "      print((f\"Epoch {t+1}: \" if verbosity >= 3 else \"\") + f\"mean loss {mean_loss}, validation accuracy {accuracy:.2%}\")\n",
        "    if verbosity >= 3:\n",
        "      print(\"\\n\")\n",
        "  \n",
        "  if verbosity >= 1:\n",
        "    print(f\"Training complete, least loss {least_loss}, final validation accuracy {accuracy:.2%}\")\n",
        "  return least_loss"
      ],
      "metadata": {
        "id": "FgKbUBdWMAm1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_dataloader = torch.utils.data.DataLoader(dataset, batch_size=4, shuffle=True)\n",
        "loss_fn = nn.BCELoss()\n",
        "optimiser = torch.optim.Adam(net.parameters(), lr=5e-3)\n",
        "\n",
        "least_loss = train(training_dataloader, net, loss_fn, optimiser, epochs=250, verbosity=2)\n"
      ],
      "metadata": {
        "id": "Qv36ZZ4aXkmA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Okay, this is encouraging. We are getting a loss in the order of $10^{-3}$ (which is relatively little for mean binary cross-entropy) and 100% accuracy. You will notice that as the training progresses, the loss tends to decrease and the accuracy increases. You will also notice that our neural network has only one hidden layer of 256 neurons. But are all those neurons really necessary?\n",
        "\n",
        "Let's push things to an extremum and consider a network that has exactly one neuron in its hidden layer. In other words, all of the information about the input the output neurons have must be contained in exactly one activated number, and the hidden layer of such a network is an information bottleneck. All other parameters constant, what sort of loss values and accuracies will we be getting under such circumstances?"
      ],
      "metadata": {
        "id": "kkVWsgZU36Bg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "slender_net = DeepNeuralNet(input_width=8, hidden_layer_profile=[ 1 ], output_width=8, output_activation=nn.Sigmoid())\n",
        "\n",
        "training_dataloader = torch.utils.data.DataLoader(dataset, batch_size=4, shuffle=True)\n",
        "loss_fn = nn.BCELoss()\n",
        "optimiser = torch.optim.Adam(slender_net.parameters(), lr=5e-3)\n",
        "\n",
        "least_slender_loss = train(training_dataloader, slender_net, loss_fn, optimiser, epochs=250, verbosity=1)"
      ],
      "metadata": {
        "id": "_nIrvnj98dN5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that with one hidden neuron, we learn to predict the values of $f(x)$ only marginally better than a coin flip, and that this corresponds to a relatively large value of the binary cross-entropy loss.\n",
        "\n",
        "Okay. So there is a number of hidden neurons (256) that is sufficient for learning $f$ with 100% accuracy, and there is a number of hidden neurons (1) that is clearly insufficient to learn anything but some rough indication of the correct output. \n",
        "\n",
        "*   With one hidden neuron, we have starved the network of representational power to the extent that it is only slightly better than tossing a fair coin at predicting $f$.\n",
        "*   With 256 hidden neurons, we have given the network enough representational power to learn $f$. Perhaps even too much.\n",
        "\n",
        "What happens in between these two extrema? And, is there a point - a number of neurons - beyond which the network fails to learn $f$ correctly but for which $f$ can still be learned?"
      ],
      "metadata": {
        "id": "zFLICSAFATyE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise.** Find the least number of neurons $w_1$ such that the training of a shallow neural network with $w$ hidden neurons can still learn to execute $f$ with loss of at most $0.001$. Remember that you can adjust the learning rate and the number of epochs to get finer and more resource-efficient training. You can also set `verbosity=1` to avoid long listings of losses, though verbosity of above `1` might help with the investigation of whether the training losses plateau out."
      ],
      "metadata": {
        "id": "o0wKjLNG4Yrn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w = None\n",
        "net = DeepNeuralNet(input_width=8, hidden_layer_profile=[ w_1 ], output_width=8, output_activation=nn.Sigmoid())\n",
        "\n",
        "training_dataloader = torch.utils.data.DataLoader(dataset, batch_size=4, shuffle=True)\n",
        "loss_fn = nn.BCELoss()\n",
        "optimiser = torch.optim.Adam(net.parameters(), lr=5e-3)\n",
        "least_loss = train(training_dataloader, net, loss_fn, optimiser, epochs=250, verbosity=2)"
      ],
      "metadata": {
        "id": "ATMk4IcY4vqb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise.** It has been theoretically proven that deep neural networks (that is, networks with more than one hidden layer) can learn the same functions as shallow neural nets while using comparatively fewer neurons and trainable weights. Can you find a `w_2`, a minimal number of neurons sufficient to learn the function $f$ with loss of at most $0.001$, such that the $w_2$ neurons can be distributed in multiple hidden layers? The number of layers you end up using is up to you."
      ],
      "metadata": {
        "id": "j2XD3gXi5GdP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "net = DeepNeuralNet(input_width=8, hidden_layer_profile=[ None, None, None, ], output_width=8, output_activation=nn.Sigmoid())\n",
        "\n",
        "training_dataloader = torch.utils.data.DataLoader(dataset, batch_size=4, shuffle=True)\n",
        "loss_fn = nn.BCELoss()\n",
        "optimiser = torch.optim.Adam(net.parameters(), lr=5e-3)\n",
        "least_loss = train(training_dataloader, net, loss_fn, optimiser, epochs=250, verbosity=2)"
      ],
      "metadata": {
        "id": "uT-6dQcE6UQw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Section Takeaway\n",
        "\n",
        "We have seen that neural networks can be constructed as directed graphs of more elementary building blocks (shallow and deep nets) and other networks (two-tower nets).\n",
        "\n",
        "We have also introduced the training loop for a neural network that uses much of PyTorch machinery to perform gradient descent.\n",
        "\n",
        "We have used the above to train networks that learn a Boolean function. We observed that not all networks can learn all functions, and that the number of neurons and their arrangement (or, more precisely, trainable weights and their role within the network) influence the ability of a network to learn to approximate a function. Experimenting, we got the intuitive feel of the notion of *representational power*.\n",
        "\n",
        "Using all of that has been learned, we can now go and train networks that are perhaps more suitable for real-world applications."
      ],
      "metadata": {
        "id": "DtlSN3BgDObv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Image Classification with DNNs\n"
      ],
      "metadata": {
        "id": "CgPbCBVh-X4A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most machine learning workflows involve working with data, creating models, optimizing model parameters, and saving the trained models. This section introduces you to a complete ML workflow implemented in PyTorch, with links to learn more about each of these concepts.\n",
        "\n",
        "We will use the FashionMNIST dataset to train a neural network that predicts if an input image belongs to one of the following classes: T-shirt/top, Trouser, Pullover, Dress, Coat, Sandal, Shirt, Sneaker, Bag, or Ankle boot."
      ],
      "metadata": {
        "id": "e-XRALwj3OQO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Working with Data\n",
        "\n",
        "PyTorch has two primitives to work with data: `torch.utils.data.DataLoader` and `torch.utils.data.Dataset`. `Dataset` stores the samples and their corresponding labels, and `DataLoader` wraps an iterable around the `Dataset`."
      ],
      "metadata": {
        "id": "3QE2rLR_3UYN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import Compose, ToTensor, Lambda\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "UrJOdfWg3fyO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PyTorch offers domain-specific libraries such as TorchText, TorchVision, and TorchAudio, all of which include datasets. For this tutorial, we will be using a TorchVision dataset.\n",
        "\n",
        "The `torchvision.datasets` module contains `Dataset` objects for many real-world vision data like CIFAR, COCO. In this tutorial, we use the `FashionMNIST` dataset. Every TorchVision `Dataset` includes two arguments: `transform` and `target_transform`, to modify the samples and labels respectively."
      ],
      "metadata": {
        "id": "mXhX0V6x3fOv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download training data from open datasets.\n",
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=Compose([\n",
        "      ToTensor(),\n",
        "      Lambda(lambda x: torch.flatten(x, start_dim=0))\n",
        "    ]),\n",
        ")\n",
        "\n",
        "# Download test data from open datasets.\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=Compose([\n",
        "      ToTensor(),\n",
        "      Lambda(lambda x: torch.flatten(x, start_dim=0))\n",
        "    ]),\n",
        ")"
      ],
      "metadata": {
        "id": "XlvqVUin3sJK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We pass the `Dataset` as an argument to `DataLoader`. This wraps an iterable over our dataset, and supports automatic batching, sampling, shuffling and multiprocess data loading. Here we define a batch size of 64, i.e. each element in the dataloader iterable will return a batch of 64 features and labels."
      ],
      "metadata": {
        "id": "1oPKr9xq30BL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "\n",
        "# Create data loaders.\n",
        "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
        "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
        "\n",
        "for X, y in test_dataloader:\n",
        "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
        "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
        "    break"
      ],
      "metadata": {
        "id": "CJWNfm3t4wDC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also have a quick peek at the data"
      ],
      "metadata": {
        "id": "TcuLF_FcYtQS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "images, labels = next(iter(train_dataloader))\n",
        "print('Shape of input tensor:', list(images.shape))\n",
        "ii = torch.reshape(images[0],(28,28))\n",
        "plt.imshow(ii, cmap='gray')\n",
        "plt.show()\n",
        "print('Label: ', int(labels[0]))"
      ],
      "metadata": {
        "id": "klZSCJp2YsdR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating Models\n",
        "As we have in previous sections when learning Boolean functions, to define a neural network in PyTorch we create a class that inherits from `nn.Module`. We define the layers of the network in the `__init__` function (the constructor) and specify how data will pass through the network in the forward function. To accelerate operations in the neural network, we move it to the GPU if available."
      ],
      "metadata": {
        "id": "q1sUIuMb37Is"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get cpu or gpu device for training.\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "net = DeepNeuralNet(input_width=28 * 28, hidden_layer_profile=[512, 512], output_width=10).to(device)\n",
        "print(net)"
      ],
      "metadata": {
        "id": "VMpr9W3b34sp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Optimising Model Parameters"
      ],
      "metadata": {
        "id": "OuUe4bYz4sKm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As illustrated before, to train a model, we need a loss function and an optimiser."
      ],
      "metadata": {
        "id": "z5KVrwn14vwB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimiser = torch.optim.SGD(net.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "sDYCYpIM4t2n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The training loop from above will serve us well even in our current tasks. We also check the model’s performance against the test dataset to ensure it is learning -- in order to do so, we re-define the `testing_loop` function we used to learn Boolean functions in the new context of image classification."
      ],
      "metadata": {
        "id": "ks-kMVbH45MH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def testing_loop(dataloader, net,):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    net.eval()\n",
        "    correct = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = net(X)\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    \n",
        "    return correct / size"
      ],
      "metadata": {
        "id": "o_HRTMXS42fX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The training process is conducted over several iterations (epochs). During each epoch, the model learns parameters to make better predictions. We print the model's accuracy and loss at each epoch; we would like to see the accuracy increase and the loss decrease with every epoch."
      ],
      "metadata": {
        "id": "riWKYtev5MGt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 5\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    training_loop(train_dataloader, net, loss_fn, optimiser)\n",
        "    validation_accuracy = testing_loop(train_dataloader, net)\n",
        "    print(f\"Validation Accuracy: {validation_accuracy:.2%}\\n\")\n",
        "print(\"Training Done!\")\n",
        "\n",
        "testing_accuracy = testing_loop(test_dataloader, net)\n",
        "print(f\"\\nTest Accuracy: {testing_accuracy:.2%}\")"
      ],
      "metadata": {
        "id": "Sdns1QKu5LK4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise.** Play around with different learning setups. Modifying just the learning rate and the number of epochs, how high can you take the validation accuracy? While doing so, do you also observe similar improvements in the test accuracy? (Remember that you can adjust the verbosity level not to have to read through long listings)."
      ],
      "metadata": {
        "id": "ErIgSMFqLw_p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train(train_dataloader, net, loss_fn, optimiser, epochs, verbosity=3)"
      ],
      "metadata": {
        "id": "IlxBrCs6L3tb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise.** Play around with the architecture of the neural network that you train. Adding more layers and more neurons, can you take the test performance even higher than in the previous exercise?"
      ],
      "metadata": {
        "id": "Hdz2qeSpQll8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define the architecture of your neural network\n",
        "best_net = DeepNeuralNet(input_width=28 * 28, hidden_layer_profile=[ None ], output_width=10).to(device)\n",
        "print(best_net)\n",
        "\n",
        "# train it\n",
        "train(train_dataloader, net, loss_fn, optimiser, epochs, verbosity=3)\n",
        "\n",
        "# test it\n",
        "testing_accuracy = testing_loop(test_dataloader, net)\n",
        "print(f\"\\nTest Accuracy: {testing_accuracy:.2%}\")"
      ],
      "metadata": {
        "id": "lZbTBJlKRJQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Saving and Loading Models\n",
        "Quite often you want to save your model, either to be later deployed in practice (on a website or in a mobile device, for example), or to be able to evaluate it later, in a different workflow. A common way to save a model is to serialise the internal state dictionary (containing the model parameters)."
      ],
      "metadata": {
        "id": "LPpFsu8F59xI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(net.state_dict(), \"model.pth\")\n",
        "print(\"Saved PyTorch Model State to model.pth\")"
      ],
      "metadata": {
        "id": "iH2lJVkk5ROQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The process for loading a model includes re-creating the model structure and loading the state dictionary into it."
      ],
      "metadata": {
        "id": "_aei7Jih6FyU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = DeepNeuralNet(28 * 28, [512, 512], 10)\n",
        "model.load_state_dict(torch.load(\"model.pth\"))"
      ],
      "metadata": {
        "id": "Pa519J6y6JR8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This model can now be used to make predictions."
      ],
      "metadata": {
        "id": "aMSKxvms6ROa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classes = [\n",
        "    \"T-shirt/top\",\n",
        "    \"Trouser\",\n",
        "    \"Pullover\",\n",
        "    \"Dress\",\n",
        "    \"Coat\",\n",
        "    \"Sandal\",\n",
        "    \"Shirt\",\n",
        "    \"Sneaker\",\n",
        "    \"Bag\",\n",
        "    \"Ankle boot\",\n",
        "]\n",
        "\n",
        "model.eval()\n",
        "x, y = test_data[0][0], test_data[0][1]\n",
        "with torch.no_grad():\n",
        "    pred = model(x)\n",
        "    predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
        "    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')"
      ],
      "metadata": {
        "id": "sQFBDr7V6QOm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MNIST from Scratch\n",
        "\n",
        "You have now seen the entire pipeline, going from the exploration of training data to the model evaluation.\n",
        "\n",
        "The final task for today is to use your new knowledge to get a running model that can classify MNIST dataset digits."
      ],
      "metadata": {
        "id": "ZhE7e1KYa15l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = datasets.MNIST(\n",
        "    root='data',\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=Compose([\n",
        "      ToTensor(),\n",
        "      Lambda(lambda x: torch.flatten(x, start_dim=0))\n",
        "    ]),\n",
        ")\n",
        "\n",
        "test_data = datasets.MNIST(\n",
        "    root='data',\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=Compose([\n",
        "      ToTensor(),\n",
        "      Lambda(lambda x: torch.flatten(x, start_dim=0))\n",
        "    ]),\n",
        ")"
      ],
      "metadata": {
        "id": "C21OVbIaclaJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hint: you can follow and re-use the above code step-by-step."
      ],
      "metadata": {
        "id": "5ek1dsFqhPt5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once you have a working instance, try varying the learning rate, batch size, and the sizes and numbers of individual layers in your network in order to get the best possible training accuracy. You can share your notebook with us at the end of the session -- the two individuals or teams with the best-performing training setup will win a sweet reward."
      ],
      "metadata": {
        "id": "VG30Hk6WhUjb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Submission\n",
        "In the following cell, specify your ETH-Username. Make sure that it is spelled correctly so that your submission gets recorded under your name."
      ],
      "metadata": {
        "id": "u6ofEWhQod_t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%env ETH_USERNAME=your-username"
      ],
      "metadata": {
        "id": "EMA-90JIospt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%env ETH_USERNAME=besterma"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vjUCZNGWo3Cf",
        "outputId": "f90c0b19-b473-4c9e-a7d7-18eaed19917a"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: ETH_USERNAME=besterma\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!type -p curl >/dev/null || sudo apt install curl -y\n",
        "!curl -fsSL https://cli.github.com/packages/githubcli-archive-keyring.gpg | sudo dd of=/usr/share/keyrings/githubcli-archive-keyring.gpg \\\n",
        "&& sudo chmod go+r /usr/share/keyrings/githubcli-archive-keyring.gpg \\\n",
        "&& echo \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/githubcli-archive-keyring.gpg] https://cli.github.com/packages stable main\" | sudo tee /etc/apt/sources.list.d/github-cli.list > /dev/null \\\n",
        "&& sudo apt update \\\n",
        "&& sudo apt install gh -y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XjTLcVNEurTE",
        "outputId": "335be7d8-23eb-4804-964e-f8af448593ac"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4+1 records in\n",
            "4+1 records out\n",
            "2270 bytes (2.3 kB, 2.2 KiB) copied, 0.129697 s, 17.5 kB/s\n",
            "Get:1 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]\n",
            "Get:2 https://cli.github.com/packages stable InRelease [3,917 B]\n",
            "Get:3 https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/ InRelease [3,622 B]\n",
            "Ign:4 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu2004/x86_64  InRelease\n",
            "Hit:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease\n",
            "Hit:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu2004/x86_64  Release\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu focal InRelease\n",
            "Get:8 https://cli.github.com/packages stable/main amd64 Packages [344 B]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]\n",
            "Hit:10 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal InRelease\n",
            "Get:11 http://archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB]\n",
            "Hit:12 http://ppa.launchpad.net/cran/libgit2/ubuntu focal InRelease\n",
            "Get:13 http://security.ubuntu.com/ubuntu focal-security/universe amd64 Packages [1,000 kB]\n",
            "Hit:15 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal InRelease\n",
            "Hit:16 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu focal InRelease\n",
            "Hit:17 http://ppa.launchpad.net/ubuntugis/ppa/ubuntu focal InRelease\n",
            "Get:18 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [2,972 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1,299 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu focal-updates/restricted amd64 Packages [2,069 kB]\n",
            "Fetched 7,683 kB in 2s (3,099 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "21 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-510\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "The following NEW packages will be installed:\n",
            "  gh\n",
            "0 upgraded, 1 newly installed, 0 to remove and 21 not upgraded.\n",
            "Need to get 10.4 MB of archives.\n",
            "After this operation, 40.3 MB of additional disk space will be used.\n",
            "Get:1 https://cli.github.com/packages stable/main amd64 gh amd64 2.23.0 [10.4 MB]\n",
            "Fetched 10.4 MB in 0s (34.7 MB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package gh.\n",
            "(Reading database ... 128126 files and directories currently installed.)\n",
            "Preparing to unpack .../archives/gh_2.23.0_amd64.deb ...\n",
            "Unpacking gh (2.23.0) ...\n",
            "Setting up gh (2.23.0) ...\n",
            "Processing triggers for man-db (2.9.1-1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%env GH_TOKEN=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"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g2MqDYXRu5RQ",
        "outputId": "f48c2157-ce61-4bf7-bc7d-7fb59cd7d0a5"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: GH_TOKEN=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\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /tmp\n",
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nFCiZblhq77T",
        "outputId": "1012d94f-4f72-4e77-dbaf-052744213472"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/tmp\n",
            "/tmp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /tmp\n",
        "%env REPO_NAME=HODL_Test\n",
        "%env NB_NAME=HODL_Introduction.ipynb\n",
        "!rm -rf HODL_Test\n",
        "!gh repo clone https://hodl-student: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@github.com/besterma/$REPO_NAME.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93Iuk34Dz9ib",
        "outputId": "4947fd92-2ad5-4cde-cfc6-bdb80736bf11"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/tmp\n",
            "env: REPO_NAME=HODL_Test\n",
            "Cloning into 'HODL_Test'...\n",
            "remote: Enumerating objects: 21, done.\u001b[K\n",
            "remote: Counting objects: 100% (21/21), done.\u001b[K\n",
            "remote: Compressing objects: 100% (14/14), done.\u001b[K\n",
            "remote: Total 21 (delta 7), reused 11 (delta 4), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (21/21), 22.41 KiB | 214.00 KiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git remote set-url origin https://hodl-student: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@github.com/besterma/HODL_Test.git"
      ],
      "metadata": {
        "id": "rvdIAHDT8E5J"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/$REPO_NAME\n",
        "!git config --global user.email \"$ETH_USERNAME@ethz.ch\"\n",
        "!git config --global user.name \"$ETH_USERNAME\"\n",
        "!git checkout -b $ETH_USERNAME\n",
        "!mkdir /tmp/upload\n",
        "%cd /tmp/upload\n",
        "from google.colab import files\n",
        "file = files.upload()\n",
        "with open(\"/tmp/$REPO_NAME/$NB_NAME.ipynb\", \"wb\") as f:\n",
        "  f.write(list(file.values())[0])\n",
        "%cd /tmp/$REPO_NAME/\n",
        "!rm -r /tmp/upload/ \n",
        "\n",
        "!git add .\n",
        "!git commit -m \"Submission at $(date +%s)\"\n",
        "!git push --set-upstream origin $ETH_USERNAME\n",
        "!gh pr create --title \"Submission for $ETH_USERNAME\" --body \"Submission\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bg8nnBch0a_w",
        "outputId": "47ed65c0-758e-49a4-9321-1e8920d5306e"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: A branch named 'hold-student-1' already exists.\n",
            "On branch hold-student-1\n",
            "nothing to commit, working tree clean\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git push --set-upstream origin hold-student-1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pBvl4wtO8wiR",
        "outputId": "523af0d3-6254-4de5-8c5e-e68e870badda"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enumerating objects: 3, done.\n",
            "Counting objects:  33% (1/3)\rCounting objects:  66% (2/3)\rCounting objects: 100% (3/3)\rCounting objects: 100% (3/3), done.\n",
            "Delta compression using up to 2 threads\n",
            "Compressing objects:  50% (1/2)\rCompressing objects: 100% (2/2)\rCompressing objects: 100% (2/2), done.\n",
            "Writing objects:  50% (1/2)\rWriting objects: 100% (2/2)\rWriting objects: 100% (2/2), 254 bytes | 254.00 KiB/s, done.\n",
            "Total 2 (delta 1), reused 0 (delta 0)\n",
            "remote: Resolving deltas:   0% (0/1)\u001b[K\rremote: Resolving deltas: 100% (1/1)\u001b[K\rremote: Resolving deltas: 100% (1/1), completed with 1 local object.\u001b[K\n",
            "remote: \n",
            "remote: Create a pull request for 'hold-student-1' on GitHub by visiting:\u001b[K\n",
            "remote:      https://github.com/besterma/HODL_Test/pull/new/hold-student-1\u001b[K\n",
            "remote: \n",
            "To https://github.com/besterma/HODL_Test.git\n",
            " * [new branch]      hold-student-1 -> hold-student-1\n",
            "Branch 'hold-student-1' set up to track remote branch 'hold-student-1' from 'origin'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gh pr create --title \"Test Submission\" --body \"Submission for HODL\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9F8UEn3C7c69",
        "outputId": "e4b63f90-ccda-4631-ac53-1ccaca2e1852"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l\r\u001b[K\r\u001b[36m⣾\u001b[0m\r\u001b[K\r\u001b[36m⣽\u001b[0m\u001b[?25h\r\u001b[K\n",
            "Creating pull request for \u001b[0;36mhold-student-1\u001b[0m into \u001b[0;36mmain\u001b[0m in besterma/HODL_Test\n",
            "\n",
            "\u001b[Khttps://github.com/besterma/HODL_Test/pull/2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "L-5A3-GDoPnu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "file = files.upload()\n",
        "with open(\"/content/HODL_Test/HODL_Introduction_t.ipynb\", \"wb\") as f:\n",
        "  f.write(list(file.values())[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "u0c8QZT_VbjW",
        "outputId": "e903c531-fec6-493d-d166-da361dd2f437"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-34da4e68-053f-46d8-8e4d-505637df199c\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-34da4e68-053f-46d8-8e4d-505637df199c\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Copy_of_HODL_Introduction.ipynb to Copy_of_HODL_Introduction (1).ipynb\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-73-05cf0f86ddb6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/HODL_Test/HODL_Introduction_t.ipynb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jmVdtfBKnvD5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}